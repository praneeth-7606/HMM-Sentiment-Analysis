{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4je-ekC1l6iU"
   },
   "source": [
    "# Hidden Markov Model project\n",
    "## Machine Learning Fundamentals\n",
    "\n",
    "|First name|Last name|Master program|Contribution|\n",
    "|----------|---------|--------------|-------------|\n",
    "|BEGGARI|Islem|MLDM|25%|\n",
    "|BENGUEZZOU|Idriss|DSC|25%|\n",
    "|BOUABID|Iness|DSC|25%|\n",
    "|MEZIANE|Ghilas|DSC|25%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# WARNING : you need version 0.2.6 of hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9554,
     "status": "ok",
     "timestamp": 1677665807645,
     "user": {
      "displayName": "Richard Ser",
      "userId": "01935729326923979164"
     },
     "user_tz": -60
    },
    "id": "-cnVa0zdl-0T",
    "outputId": "3830e7b9-1012-4db9-916e-0428cc77dc42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall hmmlearn == 0.2.6\n",
    "import hmmlearn\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING : check that the hmmlearn installed is version 0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show hmmlearn  # check that the version installed is 0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyj9Ac1Wlz1m"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt  # show graph\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEiMw2lwmhy1"
   },
   "source": [
    "## In this notebook we will look at the NER dataset and use it to understand HMM and also construct a POS tagger at the same time.\n",
    "\n",
    "### Data Description:\n",
    "#### sentence: this column donates to which sentence the word belongs\n",
    "#### Word: the word in the sentence\n",
    "#### POS: Associated POS tag for the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GARTR2z4scYU"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYkRq11CsfbJ"
   },
   "source": [
    "If you imported the dataset \"NER dataset.csv\" to you google drive, you can use the following to mount and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8PDOUWFsoiI"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# data = pd.read_csv(\"/content/drive/MyDrive/MLF_project/data/NER dataset.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_wSo_y2sv6f"
   },
   "source": [
    "Otherwise, if you are working locally or if you just uploaded the dataset for this session on the drive, you can use the following to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK3Tsr14s8Oi"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/NER dataset.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV2vxuIdmGW0"
   },
   "outputs": [],
   "source": [
    "data = data.fillna(method=\"ffill\")\n",
    "data = data.rename(columns={'Sentence #': 'sentence'})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bieothyVtLxy"
   },
   "source": [
    "# Data pre-processing\n",
    "If you want to do some pre-processing (lowercase any words, remove stop words, replace numbers/names by a unique NUM/NAME token, etc.) you can do it here in the pipeline.\n",
    "\n",
    "Note : you could create a new dataset `data_pre_precessed = pre_process(data)` to keep both version and compare the effect of you pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txb_5PPstsKc"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def pre_processing(data):\n",
    "    # Convert to lower case\n",
    "    data['Word'] = data['Word'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    data['Word'] = data['Word'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "    # Remove numerical values\n",
    "    data['Word'] = data['Word'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data['Word'] = data['Word'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    # Stemming\n",
    "    # ps = PorterStemmer()\n",
    "    # data['Word'] = data['Word'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "\n",
    "    # # Lemmatization\n",
    "    # lem = WordNetLemmatizer()\n",
    "    # data['Word'] = data['Word'].apply(lambda x: ' '.join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIPM3ZblnIfB"
   },
   "source": [
    "First let's collect the unique words and the unique POS tags in the dataset, we will use this to construct the HMM later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1677617727116,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "z8ztujKumbQb",
    "outputId": "a49eec7c-0a8d-46f4-8299-a183f9d5c447"
   },
   "outputs": [],
   "source": [
    "data = pre_processing(data)\n",
    "\n",
    "tags = list(set(data.POS.values))  # Unique POS tags in the dataset\n",
    "words = list(set(data.Word.values))  # Unique words in the dataset\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXihLYTNnV84"
   },
   "source": [
    "### We have 42 different tags and 35,178 different words, so the HMM that we construct will have the following properties\n",
    "- The hidden states of the this HMM will correspond to the POS tags, so we will have 42 hidden states.\n",
    "- The Observations for this HMM will correspond to the sentences and their words.\n",
    "\n",
    "#### Before constructing the HMM, we will split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xb_rvZ6nQNH"
   },
   "outputs": [],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhZFM-48t3OI"
   },
   "outputs": [],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMrZCuzut6ef"
   },
   "outputs": [],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu1fwHIzo0KU"
   },
   "source": [
    "Now lets encode the POS and Words to be used to generate the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1677617748855,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "yj8cnwcOoznK",
    "outputId": "5d4c0212-d217-4b6f-df13-ae6267a46b16"
   },
   "outputs": [],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN'\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koiN38BSpNZb"
   },
   "source": [
    "In your theory classes you might have seen that the Hidden Markov Models can be learned by using the Baum-Welch algorithm by just using the observations.\n",
    "Although we can learn the Hidden States (POS tags) using Baum-Welch algorithm,We cannot map them back the states (words) to the POS tag. So for this exercise we will skip using the BW algorithm and directly create the HMM.\n",
    "\n",
    "For creating the HMM we should build the following three parameters. \n",
    "- `startprob_`\n",
    "- `transmat_`\n",
    "- `emissionprob_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RXWyEXlqD0B"
   },
   "source": [
    "To construct the above mentioned paramters let's first create some useful matrices that will assist us in creating the above three parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiXtl641o76N"
   },
   "outputs": [],
   "source": [
    "count_tags = dict(data_train.POS.value_counts())  # Total number of POS tags in the dataset\n",
    "# Now let's create the tags to words count\n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
    "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "# We shall also collect the counts for the first tags in the sentence\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S67MQmn5sCWJ"
   },
   "source": [
    "Now Let's build the parameter matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPV6pioBqaey"
   },
   "outputs": [],
   "source": [
    "startprob = np.zeros((len(tags),))\n",
    "transmat = np.zeros((len(tags), len(tags)))\n",
    "emissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbca__ogusIu"
   },
   "source": [
    "## Task 1: Similar to how we built the hidden state transition probability matrix as shown above, you will built the transition probability between the words. With this matrix write a function that can calculate the log likelihood given a sentence.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block creates a transition probability matrix, which stores the probability of each word transitioning to the next word in a sentence. The input data is a set of sentences, which is split into individual words. The `word2id` dictionary is used to map each word to a unique ID. \n",
    "\n",
    "Then, a count matrix is created that stores the frequency of transitions between words. The count matrix is normalized to get the transition probability matrix. The resulting matrix can be used to predict the probability of a given word transitioning to the next word in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping that stores the frequency of transitions in words to their next words\n",
    "count_words_to_next_words = np.zeros((len(words), len(words)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "words_list = list(data_train.Word)\n",
    "\n",
    "for i in range(1, len(sentences)):\n",
    "    if (sentences[i] == sentences[i-1]):\n",
    "        prevword = words_list[i-1]\n",
    "        nextword = words_list[i]\n",
    "        if prevword in word2id and nextword in word2id:\n",
    "            prevwordid = word2id[prevword]\n",
    "            nextwordid = word2id[nextword]\n",
    "            count_words_to_next_words[prevwordid][nextwordid] += 1\n",
    "\n",
    "# Normalize the count of transitions to their next words to obtain a transition probability matrix.\n",
    "word_transition_matrix = np.zeros((len(words), len(words)))\n",
    "sum_words_to_next_words = np.sum(count_words_to_next_words, axis=1)\n",
    "\n",
    "# Iterate over all words and compute the probability of transitioning to each possible next word.\n",
    "# If a word has no transitions to any other words, leave its row in the transition matrix as zeros.\n",
    "for i in tqdm(range(len(words)), position=0, leave=True):\n",
    "    if sum_words_to_next_words[i] > 0:\n",
    "        word_transition_matrix[i] = count_words_to_next_words[i] / sum_words_to_next_words[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the log likelihood of a given sentence based on a word transition matrix.\n",
    "It first converts the sentence to a list of word ids using a dictionary mapping of words to ids.\n",
    "Then it iterates through the list of word ids, calculating the log likelihood of each transition\n",
    "based on the word transition matrix. \n",
    "\n",
    "Finally, it returns the total log likelihood of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1677617755978,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "Quj1lpeeurXC",
    "outputId": "0d28b30d-574f-47b1-be21-32a659b9d236"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def calculate_log_likelihood(sentence: List[str], word_transition_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Given a sentence and word_transition_matrix, returns the log likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    # Convert the sentence to a list of word ids\n",
    "    word_ids = [word2id.get(word, word2id['UNKNOWN']) for word in sentence]\n",
    "    \n",
    "    # Calculate the log likelihood using the word transition matrix\n",
    "    log_likelihood = 0.0\n",
    "    for i in range(1, len(word_ids)):\n",
    "        prev_word_id = word_ids[i-1]\n",
    "        curr_word_id = word_ids[i]\n",
    "        log_likelihood += np.log(word_transition_matrix[prev_word_id, curr_word_id])\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some test given by **Mr. Richard Serrano**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_log_likelihood([\"This\", \"is\", \"a\", \"protest\", \"about\", \"how\", \"the\", \"new\", \"law\", \"is\", \"not\", \"in\", \"the\", \"interest\", \"of\", \"the\", \"people\"], word_transition_matrix)) # = -70.96\n",
    "print(calculate_log_likelihood([\"The\",\"international\",\"conference\",\"will\",\"continue\",\"as\",\"planned\",\"on\",\"Friday\"], word_transition_matrix)) # = -37.09\n",
    "print(calculate_log_likelihood([\"Who\", \"are\", \"you\", \"?\"], word_transition_matrix)) # = -15.99\n",
    "print(calculate_log_likelihood([\"You\", \"are\", \"not\", \"me\"], word_transition_matrix)) # = -13.90\n",
    "print(calculate_log_likelihood([\"Do\", \"you\", \"expect\", \"to\", \"be\", \"happy\", \"to\", \"work\", \"late\"], word_transition_matrix)) # = -35.12\n",
    "print(calculate_log_likelihood([\"This\", \"is\", \"a\", \"test\", \"sentence\"], word_transition_matrix)) # = -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edf1YAFvwgXV"
   },
   "source": [
    "#### Now we will continue to constructing the HMM.\n",
    "\n",
    "We will use the hmmlearn implementation to initialize the HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjanyLMiwzPa"
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYDvk4HEw_SO"
   },
   "source": [
    "#### Before using the HMM to predict the POS tags, we have to fix the training set as some of the words and tags in the test data might not appear in the training data so we collect this data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_GGsS_-w-fc"
   },
   "outputs": [],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "\n",
    "# TODO use panda solution\n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3UGvRN9x2fn"
   },
   "source": [
    "Now that we have the HMM ready lets predict the best path from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181278,
     "status": "ok",
     "timestamp": 1677617955431,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "_5PGjZaXx6xS",
    "outputId": "7acd6eb7-12d3-45fd-94b1-30fc0fd1b25f"
   },
   "outputs": [],
   "source": [
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CZUVE2n1BVI"
   },
   "source": [
    "The hmmlearn predict function will give the best probable path for the given sentence using the Viterbi algorithm.\n",
    "\n",
    "## Task 2: Using the model parameters (startprob_, transmat_, emissionprob_) write the viterbi algorithm from scratch to calculate the best probable path and compare it with the hmmlearn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G279w_RtjJ-"
   },
   "source": [
    "Now before using these matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUzYlQjHqcXH"
   },
   "outputs": [],
   "source": [
    "def Viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array:\n",
    "    \"\"\"\n",
    "    Write the viterbi algorithm from scratch to find the best probable path\n",
    "    attr:\n",
    "      pi: initial probabilities\n",
    "      a: transition probabilities\n",
    "      b: emission probabilities\n",
    "      obs: list of observations\n",
    "    return:\n",
    "      array of the indices of the best hidden states\n",
    "    \"\"\"\n",
    "    n = a.shape[0]\n",
    "    T = len(obs)\n",
    "\n",
    "    # Initialize the trellis table\n",
    "    trellis = np.zeros((n, T))\n",
    "    backpointers = np.zeros((n, T), dtype=int)\n",
    "\n",
    "    # Initialize the first column\n",
    "    trellis[:, 0] = pi * b[:, obs[0]]\n",
    "\n",
    "    # Loop over the remaining columns\n",
    "    for t in range(1, T):\n",
    "        for j in range(n):\n",
    "            # Calculate the scores for all possible previous states\n",
    "            scores = trellis[:, t-1] * a[:, j] * b[j, obs[t]]\n",
    "\n",
    "            # Choose the highest score\n",
    "            trellis[j, t] = np.max(scores)\n",
    "            backpointers[j, t] = np.argmax(scores)\n",
    "\n",
    "    # Backtrack to find the most probable sequence of hidden states\n",
    "    path = np.zeros(T, dtype=int)\n",
    "    path[-1] = np.argmax(trellis[:, -1])\n",
    "    for t in range(T-2, -1, -1):\n",
    "        path[t] = backpointers[path[t+1], t+1]\n",
    "\n",
    "    return path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function to decode a given path. This function will be used to decode the best path that we get from the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_path(path, hidden_states):\n",
    "    return [hidden_states[state_index] for state_index in path]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the best probable path found by the hmmlearn implementation and the one we found using the viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [\"This\", \"is\", \"a\", \"protest\", \"about\", \"how\", \"the\", \"new\", \"law\", \"is\", \"not\", \"in\", \"the\", \"interest\", \"of\", \"the\", \"people\"]\n",
    "obs_idx = [word2id[word] if word in word2id else word2id['UNKNOWN'] for word in obs]\n",
    "\n",
    "# With our Viterbi function we obtain the best probable path\n",
    "path_viterbi = Viterbi(model.startprob_, model.transmat_, model.emissionprob_, obs_idx)\n",
    "\n",
    "# Using the decode method from hmmlearn to obtain the best probable path\n",
    "logprob, path_hmmlearn = model.decode(np.array(obs_idx).reshape(-1, 1))\n",
    "\n",
    "# Compare the two paths\n",
    "print(f\"Viterbi path: {path_viterbi}\")\n",
    "print(f\"hmmlearn path: {path_hmmlearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnHjHcLJ4Kz1"
   },
   "source": [
    "### Task 3: Let's try to form our own HMM\n",
    "In this task you will try to formulate your own HMM. Image a toy example that you think that closely relates to a Hidden Markov Model.\n",
    "\n",
    "Steps:\n",
    " 1. Define your hidden states\n",
    " 2. Define your observable states\n",
    " 3. Randomly generate your observations\n",
    "\n",
    "Below is an example to demonstrate:\n",
    "\n",
    "-In this toy HMM example, we have two hidden states 'healthy' and 'sick' these states relate to the state of a pet. In this example we cannot exactly know the situation of the pet if it is 'healthy' or 'sick'\n",
    "\n",
    "-The observable states in this formulation is the what our pet is doing, whether it is sleeping, eating or pooping. We ideally want to determine if the pet is sick or not using these observable states\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observations = []\n",
    "for i in range(100):\n",
    "  observations.append(random.choice(observable_states))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2WAl5Pw7Oud"
   },
   "source": [
    "TASK 3: Now try to formulate your HMM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hidden_emotions = ['happy', 'angry', 'sad', 'calm', 'disgusted']\n",
    "observable_face = ['smiling', 'frowning', 'crying', 'neutral', 'grimacing']\n",
    "observable_map_face = {'smiling': 0, 'frowning': 1, 'crying': 2, 'neutral': 3, 'grimacing': 4}\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "random.seed(54545324)\n",
    "\n",
    "observations_face_emotions = []\n",
    "for i in range(100):\n",
    "    observations_face_emotions.append(observable_map_face[random.choice(observable_face)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWCJyGjS7hKp"
   },
   "source": [
    "Even tough we have generated the data randomly, for the learning purposes, let's try to learn an HMM from this data. For this we have to construct the Baum-Welch algorithm from scratch. Below is the skeleton of the Baum-Welch learning algorithm.\n",
    "\n",
    "## TASK 4: Complete the forward and backward probs functions in the Baum-Welch algorithm and try it with your formulated HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8571,
     "status": "ok",
     "timestamp": 1677623130291,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "v6mJpFUg7K2V",
    "outputId": "0929edcc-5965-41e3-8b64-971085940f91",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def baum_welch(observations, observations_vocab, n_hidden_states):\n",
    "    \"\"\"\n",
    "    Baum-Welch algorithm for estimating the HMM parameters\n",
    "    :param observations: observations\n",
    "    :param observations_vocab: observations vocabulary\n",
    "    :param n_hidden_states: number of hidden states to estimate\n",
    "    :return: a, b (transition matrix and emission matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        forward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined alpha_\n",
    "        \"\"\"\n",
    "        a_start = 1 / n_hidden_states\n",
    "        alpha_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "    \n",
    "        # Initialize alpha_0\n",
    "        alpha_[:, 0] = a_start * b_[:, np.where(observations_vocab == observations[0])[0][0]]\n",
    "\n",
    "        # Recursively calculate alpha_t for t > 0\n",
    "        for t in range(1, len(observations)):\n",
    "            for j in range(n_hidden_states):\n",
    "                alpha_[j, t] = np.sum(alpha_[:, t - 1] * a_[:, j]) * b_[j, np.where(observations_vocab == observations[t])[0][0]]\n",
    "\n",
    "        return alpha_\n",
    "\n",
    "    def backward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        backward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined beta_\n",
    "        \"\"\"\n",
    "        beta_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        beta_[:, -1:] = 1\n",
    "\n",
    "        # Recursively calculate beta_t for t < T-1\n",
    "        for t in range(len(observations) - 2, -1, -1):\n",
    "            for j in range(n_hidden_states):\n",
    "                beta_[j, t] = np.sum(a_[j, :] * b_[:, np.where(observations_vocab == observations[t+1])[0][0]] * beta_[:, t+1])\n",
    "\n",
    "        return beta_\n",
    "\n",
    "    def compute_gamma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # gamma_prob = np.zeros(n_samples, len(observations))\n",
    "        gamma_prob = np.multiply(alfa, beta) / sum(np.multiply(alfa, beta))\n",
    "        return gamma_prob\n",
    "\n",
    "    def compute_sigma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sigma_prob = np.zeros((n_samples, len(observations) - 1, n_samples), dtype=float)\n",
    "        denomenator = np.multiply(alfa, beta)\n",
    "        for i in range(len(observations) - 1):\n",
    "            for j in range(n_samples):\n",
    "                for k in range(n_samples):\n",
    "                    index_in_vocab = np.where(vocab == observations[i + 1])[0][0]\n",
    "                    sigma_prob[j, i, k] = (alfa[j, i] * beta[k, i + 1] * a_[j, k] * b_[k, index_in_vocab]) / sum(\n",
    "                        denomenator[:, j])\n",
    "        return sigma_prob\n",
    "\n",
    "    # initialize A ,B\n",
    "    a = np.ones((n_hidden_states, n_hidden_states)) / n_hidden_states\n",
    "    b = np.ones((n_hidden_states, len(observations_vocab))) / len(observations_vocab)\n",
    "    for iter in tqdm(range(2000), position=0, leave=True):\n",
    "\n",
    "        # E-step caclculating sigma and gamma\n",
    "        alfa_prob = forward_probs(observations, observations_vocab, n_hidden_states, a, b)  #\n",
    "        beta_prob = backward_probs(observations, observations_vocab, n_hidden_states, a, b)  # , beta_val\n",
    "        gamma_prob = compute_gamma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "        sigma_prob = compute_sigma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "\n",
    "        # M-step caclculating A, B matrices\n",
    "        a_model = np.zeros((n_hidden_states, n_hidden_states))\n",
    "        for j in range(n_hidden_states):  # calculate A-model\n",
    "            for i in range(n_hidden_states):\n",
    "                for t in range(len(observations) - 1):\n",
    "                    a_model[j, i] = a_model[j, i] + sigma_prob[j, t, i]\n",
    "                normalize_a = [sigma_prob[j, t_current, i_current] for t_current in range(len(observations) - 1) for\n",
    "                               i_current in range(n_hidden_states)]\n",
    "                normalize_a = sum(normalize_a)\n",
    "                if normalize_a == 0:\n",
    "                    a_model[j, i] = 0\n",
    "                else:\n",
    "                    a_model[j, i] = a_model[j, i] / normalize_a\n",
    "\n",
    "        b_model = np.zeros((n_hidden_states, len(observations_vocab)))\n",
    "\n",
    "        for j in range(n_hidden_states):\n",
    "            for i in range(len(observations_vocab)):\n",
    "                indices = [idx for idx, val in enumerate(observations) if val == observations_vocab[i]]\n",
    "                numerator_b = sum(gamma_prob[j, indices])\n",
    "                denominator_b = sum(gamma_prob[j, :])\n",
    "                if denominator_b == 0:\n",
    "                    b_model[j, i] = 0\n",
    "                else:\n",
    "                    b_model[j, i] = numerator_b / denominator_b\n",
    "\n",
    "        a = a_model\n",
    "        b = b_model\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observable_map = {'sleeping': 0, 'eating': 1, 'pooping': 2}\n",
    "observations = []\n",
    "\n",
    "for i in range(100):\n",
    "    observations.append(observable_map[random.choice(observable_states)])\n",
    "\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
    "                  n_hidden_states=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our own formulated HMM to try the ``baum_welch`` algorithm. The transition matrix and emission matrix will be used in our ```test_sentence``` to predict the most probable path with the ``Viterbi`` algorithm that we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJzrpdwJEf0c"
   },
   "outputs": [],
   "source": [
    "transmat, emission = baum_welch(observations=observations_face_emotions, observations_vocab=np.array(list(observable_map_face.values())),\n",
    "                  n_hidden_states=len(hidden_emotions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``test_sentence`` function takes a sentence as input and uses the Viterbi algorithm to predict the sequence of hidden emotions that are most likely to have generated the observable facial expressions present in the sentence.\n",
    "\n",
    "We first maps the observable facial expressions in the sentence to their corresponding indices using the ``observable_map_face`` dictionary. We then initializes the Viterbi algorithm with the start probabilities for each hidden emotion, which we set to be uniform across all emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(sentence):\n",
    "    obs_idx = [observable_map_face.get(o, -1) for o in sentence]\n",
    "    # we remove the -1 values (unrecognized words)\n",
    "    obs_idx = [o for o in obs_idx if o != -1]\n",
    "\n",
    "    # we run the Viterbi algorithm to find the most likely path\n",
    "    n_hidden_states = len(hidden_emotions)\n",
    "    startprob = np.ones(n_hidden_states) / n_hidden_states\n",
    "    path_viterbi = Viterbi(startprob, transmat, emission, obs_idx)\n",
    "\n",
    "    print(\"The sentence was: \", ' '.join(sentence))\n",
    "    print(\"The Viterbi path was: \", path_viterbi)\n",
    "    print(\"The sentiment of the sentence is: \", decode_path(path_viterbi, hidden_emotions))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our model with the test defined below. \n",
    "\n",
    "NOTE: We generated the observation sequence randomly with ``observations_face_emotions.append(observable_map_face[random.choice(observable_face)])``, so the predicted emotions may not always be accurate or meaningful. Furthermore, the observable states are only ``['smiling', 'frowning', 'crying', 'neutral', 'grimacing']``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANGRY\n",
    "test_sentence([\"I\", \"was\", \"frowning\", \"at\", \"the\", \"cat\", \"in\", \"the\", \"street\", \".\"])\n",
    "\n",
    "# HAPPY\n",
    "test_sentence([\"She\", \"was\", \"smiling\", \"at\", \"me\", \".\"])\n",
    "test_sentence([\"He\", \"grinned\", \",\", \"his\", \"face\", \"smiling\", \"with\", \"delight\", \"as\", \"he\", \"read\", \"the\", \"good\", \"news\", \".\"]) \n",
    "\n",
    "# DISGUSTED\n",
    "test_sentence([\"She\", \"couldn't\", \"hide\", \"her\", \"grimacing\", \"face\", \"as\", \"she\", \"tasted\", \"the\", \"spicy\", \"food\", \".\"])\n",
    "\n",
    "# SAD\n",
    "test_sentence([\"She\", \"was\", \"crying\", \"because\", \"she\", \"had\", \"lost\", \"her\", \"dog\", \".\"])\n",
    "\n",
    "# CALM\n",
    "test_sentence([\"He\", \"had\", \"a\", \"neutral\", \"expression\", \".\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1zewbaLQpah4tXjgkum-DIYJX04_gCbEc",
     "timestamp": 1677666490463
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e743294b9eb9579c1dbfa440bf1529440bbf17bdd5f490c626f9a31cee6b9bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
