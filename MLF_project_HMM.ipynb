{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4je-ekC1l6iU"
   },
   "source": [
    "# Hidden Markov Model project\n",
    "## Machine Learning Fundamentals\n",
    "\n",
    "|First name|Last name|Master program|Contribution|\n",
    "|----------|---------|--------------|-------------|\n",
    "|BEGGARI|Islem|MLDM|25%|\n",
    "|BENGUEZZOU|Idriss|DSC|25%|\n",
    "|BOUABID|Iness|DSC|25%|\n",
    "|MEZIANE|Ghilas|DSC|25%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# WARNING : you need version 0.2.6 of hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9554,
     "status": "ok",
     "timestamp": 1677665807645,
     "user": {
      "displayName": "Richard Ser",
      "userId": "01935729326923979164"
     },
     "user_tz": -60
    },
    "id": "-cnVa0zdl-0T",
    "outputId": "3830e7b9-1012-4db9-916e-0428cc77dc42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall hmmlearn == 0.2.6\n",
    "import hmmlearn\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING : check that the hmmlearn installed is version 0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: hmmlearn\n",
      "Version: 0.2.6\n",
      "Summary: Hidden Markov Models in Python with scikit-learn like API\n",
      "Home-page: https://github.com/hmmlearn/hmmlearn\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: c:\\users\\administrateur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\n",
      "Requires: numpy, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: #, 0.2.6, check, installed, is, that, the, version\n"
     ]
    }
   ],
   "source": [
    "!pip show hmmlearn  # check that the version installed is 0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "tyj9Ac1Wlz1m"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt  # show graph\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEiMw2lwmhy1"
   },
   "source": [
    "## In this notebook we will look at the NER dataset and use it to understand HMM and also construct a POS tagger at the same time.\n",
    "\n",
    "### Data Description:\n",
    "#### sentence: this column donates to which sentence the word belongs\n",
    "#### Word: the word in the sentence\n",
    "#### POS: Associated POS tag for the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GARTR2z4scYU"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYkRq11CsfbJ"
   },
   "source": [
    "If you imported the dataset \"NER dataset.csv\" to you google drive, you can use the following to mount and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "P8PDOUWFsoiI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/content/drive/MyDrive/MLF_project/data/NER dataset.csv\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/MLF_project/data/NER dataset.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_wSo_y2sv6f"
   },
   "source": [
    "Otherwise, if you are working locally or if you just uploaded the dataset for this session on the drive, you can use the following to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "vK3Tsr14s8Oi"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/NER dataset.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "YV2vxuIdmGW0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(method=\"ffill\")\n",
    "data = data.rename(columns={'Sentence #': 'sentence'})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bieothyVtLxy"
   },
   "source": [
    "# Data pre-processing\n",
    "If you want to do some pre-processing (lowercase any words, remove stop words, replace numbers/names by a unique NUM/NAME token, etc.) you can do it here in the pipeline.\n",
    "\n",
    "Note : you could create a new dataset `data_pre_precessed = pre_process(data)` to keep both version and compare the effect of you pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "txb_5PPstsKc"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "def pre_processing(data):\n",
    "    # Concert to lower case\n",
    "    data['Word'] = data['Word'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # # Delete the punctuation and digits    \n",
    "    # data['Word'] = data['Word'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation + string.digits), '', x))\n",
    "    \n",
    "    # # Remove the stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # data['Word'] = data['Word'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIPM3ZblnIfB"
   },
   "source": [
    "First let's collect the unique words and the unique POS tags in the dataset, we will use this to construct the HMM later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1677617727116,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "z8ztujKumbQb",
    "outputId": "a49eec7c-0a8d-46f4-8299-a183f9d5c447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 31811)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pre_processing(data)\n",
    "\n",
    "tags = list(set(data.POS.values))  # Unique POS tags in the dataset\n",
    "words = list(set(data.Word.values))  # Unique words in the dataset\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXihLYTNnV84"
   },
   "source": [
    "### We have 42 different tags and 35,178 different words, so the HMM that we construct will have the following properties\n",
    "- The hidden states of the this HMM will correspond to the POS tags, so we will have 42 hidden states.\n",
    "- The Observations for this HMM will correspond to the sentences and their words.\n",
    "\n",
    "#### Before constructing the HMM, we will split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "3Xb_rvZ6nQNH"
   },
   "outputs": [],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhZFM-48t3OI"
   },
   "outputs": [],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMrZCuzut6ef"
   },
   "outputs": [],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu1fwHIzo0KU"
   },
   "source": [
    "Now lets encode the POS and Words to be used to generate the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1677617748855,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "yj8cnwcOoznK",
    "outputId": "5d4c0212-d217-4b6f-df13-ae6267a46b16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 25094)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN'\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koiN38BSpNZb"
   },
   "source": [
    "In your theory classes you might have seen that the Hidden Markov Models can be learned by using the Baum-Welch algorithm by just using the observations.\n",
    "Although we can learn the Hidden States (POS tags) using Baum-Welch algorithm,We cannot map them back the states (words) to the POS tag. So for this exercise we will skip using the BW algorithm and directly create the HMM.\n",
    "\n",
    "For creating the HMM we should build the following three parameters. \n",
    "- `startprob_`\n",
    "- `transmat_`\n",
    "- `emissionprob_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RXWyEXlqD0B"
   },
   "source": [
    "To construct the above mentioned paramters let's first create some useful matrices that will assist us in creating the above three parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "uiXtl641o76N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702936/702936 [00:00<00:00, 718535.34it/s]\n"
     ]
    }
   ],
   "source": [
    "count_tags = dict(data_train.POS.value_counts())  # Total number of POS tags in the dataset\n",
    "# Now let's create the tags to words count\n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
    "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "# We shall also collect the counts for the first tags in the sentence\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S67MQmn5sCWJ"
   },
   "source": [
    "Now Let's build the parameter matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "LPV6pioBqaey"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 49.18it/s]\n"
     ]
    }
   ],
   "source": [
    "startprob = np.zeros((len(tags),))\n",
    "transmat = np.zeros((len(tags), len(tags)))\n",
    "emissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbca__ogusIu"
   },
   "source": [
    "## Task 1: Similar to how we built the hidden state transition probability matrix as shown above, you will built the transition probability between the words. With this matrix write a function that can calculate the log likelihood given a sentence.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25094\n",
      "(25094, 25094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrateur\\AppData\\Local\\Temp\\ipykernel_11480\\2077415391.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  word_transition_matrix = count_words_to_next_words / sum_words_to_next_words[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping that stores the frequency of transitions in words to their next words\n",
    "count_words_to_next_words = np.zeros((len(words), len(words)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "words = list(data_train.Word)\n",
    "\n",
    "for i in range(1, len(sentences)):\n",
    "    if (sentences[i] == sentences[i-1]):\n",
    "        prevword = words[i-1]\n",
    "        nextword = words[i]\n",
    "        if prevword in word2id and nextword in word2id:\n",
    "            prevwordid = word2id[prevword]\n",
    "            nextwordid = word2id[nextword]\n",
    "            count_words_to_next_words[prevwordid][nextwordid] += 1\n",
    "\n",
    "# Normalize the counts to get the transition probabilities\n",
    "sum_words_to_next_words = np.sum(count_words_to_next_words, axis=1)\n",
    "word_transition_matrix = count_words_to_next_words / sum_words_to_next_words[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#name?</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>%-plus</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>'80s</th>\n",
       "      <th>...</th>\n",
       "      <th>zurich-based</th>\n",
       "      <th>zvarych</th>\n",
       "      <th>zviad</th>\n",
       "      <th>zvornik</th>\n",
       "      <th>zwally</th>\n",
       "      <th>zyazikov</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>ï¿½</th>\n",
       "      <th>ï¿½c</th>\n",
       "      <th>ï¿½s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name?</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyazikov</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zydeco</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ï¿½</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ï¿½c</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ï¿½s</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25094 rows × 25094 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            !    \"    #  #name?    $    %  %-plus    &    '  '80s  ...  \\\n",
       "!         0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "\"         0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "#         0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "#name?    0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "$         0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "...       ...  ...  ...     ...  ...  ...     ...  ...  ...   ...  ...   \n",
       "zyazikov  0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "zydeco    0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "ï¿½       0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "ï¿½c      0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "ï¿½s      0.0  0.0  0.0     0.0  0.0  0.0     0.0  0.0  0.0   0.0  ...   \n",
       "\n",
       "          zurich-based  zvarych  zviad  zvornik  zwally  zyazikov  zydeco  \\\n",
       "!                  0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "\"                  0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "#                  0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "#name?             0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "$                  0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "...                ...      ...    ...      ...     ...       ...     ...   \n",
       "zyazikov           0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "zydeco             0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "ï¿½                0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "ï¿½c               0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "ï¿½s               0.0      0.0    0.0      0.0     0.0       0.0     0.0   \n",
       "\n",
       "          ï¿½  ï¿½c  ï¿½s  \n",
       "!         0.0   0.0   0.0  \n",
       "\"         0.0   0.0   0.0  \n",
       "#         0.0   0.0   0.0  \n",
       "#name?    0.0   0.0   0.0  \n",
       "$         0.0   0.0   0.0  \n",
       "...       ...   ...   ...  \n",
       "zyazikov  0.0   0.0   0.0  \n",
       "zydeco    0.0   0.0   0.0  \n",
       "ï¿½       0.0   0.0   0.0  \n",
       "ï¿½c      0.0   0.0   0.0  \n",
       "ï¿½s      0.0   0.0   0.0  \n",
       "\n",
       "[25094 rows x 25094 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_transition_matrix(word_transition_matrix, word2id):\n",
    "    words = sorted(list(word2id.keys()))\n",
    "    transition_df = pd.DataFrame(word_transition_matrix, index=words, columns=words)\n",
    "    return transition_df\n",
    "\n",
    "df = print_transition_matrix(word_transition_matrix, word2id)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1677617755978,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "Quj1lpeeurXC",
    "outputId": "0d28b30d-574f-47b1-be21-32a659b9d236"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def calculate_log_likelihood(sentence: List[str], word_transition_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Given a sentence and word_transition_matrix, returns the log likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    # Convert the sentence to a list of word ids\n",
    "    word_ids = [word2id.get(word, word2id['UNKNOWN']) for word in sentence]\n",
    "    \n",
    "    # Calculate the log likelihood using the word transition matrix\n",
    "    log_likelihood = 0.0\n",
    "    for i in range(1, len(word_ids)):\n",
    "        prev_word_id = word_ids[i-1]\n",
    "        curr_word_id = word_ids[i]\n",
    "        log_likelihood += np.log(word_transition_matrix[prev_word_id, curr_word_id])\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-72.43641567046959\n",
      "-36.459904595704714\n",
      "-18.26874178389201\n",
      "-16.28246985751739\n",
      "-38.588797505198656\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrateur\\AppData\\Local\\Temp\\ipykernel_11480\\1984602900.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  log_likelihood += np.log(word_transition_matrix[prev_word_id, curr_word_id])\n"
     ]
    }
   ],
   "source": [
    "print(calculate_log_likelihood([\"This\", \"is\", \"a\", \"protest\", \"about\", \"how\", \"the\", \"new\", \"law\", \"is\", \"not\", \"in\", \"the\", \"interest\", \"of\", \"the\", \"people\"], word_transition_matrix)) # = -70.96\n",
    "print(calculate_log_likelihood([\"The\",\"international\",\"conference\",\"will\",\"continue\",\"as\",\"planned\",\"on\",\"Friday\"], word_transition_matrix)) # = -37.09\n",
    "print(calculate_log_likelihood([\"Who\", \"are\", \"you\", \"?\"], word_transition_matrix)) # = -15.99\n",
    "print(calculate_log_likelihood([\"You\", \"are\", \"not\", \"me\"], word_transition_matrix)) # = -13.90\n",
    "print(calculate_log_likelihood([\"Do\", \"you\", \"expect\", \"to\", \"be\", \"happy\", \"to\", \"work\", \"late\"], word_transition_matrix)) # = -35.12\n",
    "print(calculate_log_likelihood([\"This\", \"is\", \"a\", \"test\", \"sentence\"], word_transition_matrix)) # = -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edf1YAFvwgXV"
   },
   "source": [
    "#### Now we will continue to constructing the HMM.\n",
    "\n",
    "We will use the hmmlearn implementation to initialize the HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjanyLMiwzPa"
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYDvk4HEw_SO"
   },
   "source": [
    "#### Before using the HMM to predict the POS tags, we have to fix the training set as some of the words and tags in the test data might not appear in the training data so we collect this data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_GGsS_-w-fc"
   },
   "outputs": [],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "\n",
    "# TODO use panda solution\n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3UGvRN9x2fn"
   },
   "source": [
    "Now that we have the HMM ready lets predict the best path from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181278,
     "status": "ok",
     "timestamp": 1677617955431,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "_5PGjZaXx6xS",
    "outputId": "7acd6eb7-12d3-45fd-94b1-30fc0fd1b25f"
   },
   "outputs": [],
   "source": [
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CZUVE2n1BVI"
   },
   "source": [
    "The hmmlearn predict function will give the best probable path for the given sentence using the Viterbi algorithm.\n",
    "\n",
    "## Task 2: Using the model parameters (startprob_, transmat_, emissionprob_) write the viterbi algorithm from scratch to calculate the best probable path and compare it with the hmmlearn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G279w_RtjJ-"
   },
   "source": [
    "Now before using these matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUzYlQjHqcXH"
   },
   "outputs": [],
   "source": [
    "def Viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array():\n",
    "    \"\"\"\n",
    "    Write the viterbi algorithm from scratch to find the best probable path\n",
    "    attr:\n",
    "      pi: initial probabilities\n",
    "      a: transition probabilities\n",
    "      b: emission probabilities\n",
    "      obs: list of observations\n",
    "    return:\n",
    "      array of the indices of the best hidden states\n",
    "    \"\"\"\n",
    "    # Write your function here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnHjHcLJ4Kz1"
   },
   "source": [
    "### Task 3: Let's try to form our own HMM\n",
    "In this task you will try to formulate your own HMM. Image a toy example that you think that closely relates to a Hidden Markov Model.\n",
    "\n",
    "Steps:\n",
    " 1. Define your hidden states\n",
    " 2. Define your observable states\n",
    " 3. Randomly generate your observations\n",
    "\n",
    "Below is an example to demonstrate:\n",
    "\n",
    "-In this toy HMM example, we have two hidden states 'healthy' and 'sick' these states relate to the state of a pet. In this example we cannot exactly know the situation of the pet if it is 'healthy' or 'sick'\n",
    "\n",
    "-The observable states in this formulation is the what our pet is doing, whether it is sleeping, eating or pooping. We ideally want to determine if the pet is sick or not using these observable states\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observations = []\n",
    "for i in range(100):\n",
    "  observations.append(random.choice(observable_states))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2WAl5Pw7Oud"
   },
   "source": [
    "TASK 3: Now try to formulate your HMM here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWCJyGjS7hKp"
   },
   "source": [
    "Even tough we have generated the data randomly, for the learning purposes, let's try to learn an HMM from this data. For this we have to construct the Baum-Welch algorithm from scratch. Below is the skeleton of the Baum-Welch learning algorithm.\n",
    "\n",
    "## TASK 4: Complete the forward and backward probs functions in the Baum-Welch algorithm and try it with your formulated HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8571,
     "status": "ok",
     "timestamp": 1677623130291,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "v6mJpFUg7K2V",
    "outputId": "0929edcc-5965-41e3-8b64-971085940f91",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def baum_welch(observations, observations_vocab, n_hidden_states):\n",
    "    \"\"\"\n",
    "    Baum-Welch algorithm for estimating the HMM parameters\n",
    "    :param observations: observations\n",
    "    :param observations_vocab: observations vocabulary\n",
    "    :param n_hidden_states: number of hidden states to estimate\n",
    "    :return: a, b (transition matrix and emission matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        forward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined alpha_\n",
    "        \"\"\"\n",
    "        a_start = 1 / n_hidden_states\n",
    "        alpha_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        #TODO complete the forward function to calculate alpha\n",
    "\n",
    "        return alpha_\n",
    "\n",
    "    def backward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        backward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined beta_\n",
    "        \"\"\"\n",
    "        beta_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        beta_[:, -1:] = 1\n",
    "        # TODO finish the function to calculate backward pass and calculate beta\n",
    "        return beta_\n",
    "\n",
    "    def compute_gamma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # gamma_prob = np.zeros(n_samples, len(observations))\n",
    "        gamma_prob = np.multiply(alfa, beta) / sum(np.multiply(alfa, beta))\n",
    "        return gamma_prob\n",
    "\n",
    "    def compute_sigma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sigma_prob = np.zeros((n_samples, len(observations) - 1, n_samples), dtype=float)\n",
    "        denomenator = np.multiply(alfa, beta)\n",
    "        for i in range(len(observations) - 1):\n",
    "            for j in range(n_samples):\n",
    "                for k in range(n_samples):\n",
    "                    index_in_vocab = np.where(vocab == observations[i + 1])[0][0]\n",
    "                    sigma_prob[j, i, k] = (alfa[j, i] * beta[k, i + 1] * a_[j, k] * b_[k, index_in_vocab]) / sum(\n",
    "                        denomenator[:, j])\n",
    "        return sigma_prob\n",
    "\n",
    "    # initialize A ,B\n",
    "    a = np.ones((n_hidden_states, n_hidden_states)) / n_hidden_states\n",
    "    b = np.ones((n_hidden_states, len(observations_vocab))) / len(observations_vocab)\n",
    "    for iter in tqdm(range(2000), position=0, leave=True):\n",
    "\n",
    "        # E-step caclculating sigma and gamma\n",
    "        alfa_prob = forward_probs(observations, observations_vocab, n_hidden_states, a, b)  #\n",
    "        beta_prob = backward_probs(observations, observations_vocab, n_hidden_states, a, b)  # , beta_val\n",
    "        gamma_prob = compute_gamma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "        sigma_prob = compute_sigma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "\n",
    "        # M-step caclculating A, B matrices\n",
    "        a_model = np.zeros((n_hidden_states, n_hidden_states))\n",
    "        for j in range(n_hidden_states):  # calculate A-model\n",
    "            for i in range(n_hidden_states):\n",
    "                for t in range(len(observations) - 1):\n",
    "                    a_model[j, i] = a_model[j, i] + sigma_prob[j, t, i]\n",
    "                normalize_a = [sigma_prob[j, t_current, i_current] for t_current in range(len(observations) - 1) for\n",
    "                               i_current in range(n_hidden_states)]\n",
    "                normalize_a = sum(normalize_a)\n",
    "                if normalize_a == 0:\n",
    "                    a_model[j, i] = 0\n",
    "                else:\n",
    "                    a_model[j, i] = a_model[j, i] / normalize_a\n",
    "\n",
    "        b_model = np.zeros((n_hidden_states, len(observations_vocab)))\n",
    "\n",
    "        for j in range(n_hidden_states):\n",
    "            for i in range(len(observations_vocab)):\n",
    "                indices = [idx for idx, val in enumerate(observations) if val == observations_vocab[i]]\n",
    "                numerator_b = sum(gamma_prob[j, indices])\n",
    "                denominator_b = sum(gamma_prob[j, :])\n",
    "                if denominator_b == 0:\n",
    "                    b_model[j, i] = 0\n",
    "                else:\n",
    "                    b_model[j, i] = numerator_b / denominator_b\n",
    "\n",
    "        a = a_model\n",
    "        b = b_model\n",
    "    return a, b\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observable_map = {'sleeping': 0, 'eating': 1, 'pooping': 2}\n",
    "observations = []\n",
    "for i in range(100):\n",
    "    observations.append(observable_map[random.choice(observable_states)])\n",
    "\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
    "                  n_hidden_states=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJzrpdwJEf0c"
   },
   "outputs": [],
   "source": [
    "#TASK 4: Now try it with your HMM"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1zewbaLQpah4tXjgkum-DIYJX04_gCbEc",
     "timestamp": 1677666490463
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e743294b9eb9579c1dbfa440bf1529440bbf17bdd5f490c626f9a31cee6b9bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
